{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Cjmfh6-aCq_-"
   },
   "outputs": [],
   "source": [
    "# First imports\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from glob import glob\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LCDviq2-C8Iu",
    "outputId": "4d955647-64a9-46e7-c225-059a922a2e1a"
   },
   "outputs": [],
   "source": [
    "#Delete working directory from previous experiment if exists\n",
    "fileListToDelete = ['./data/working/train', './data/working/validation', './data/working/test']\n",
    "for fileName in fileListToDelete:\n",
    "    try:\n",
    "        shutil.rmtree(fileName)\n",
    "        break\n",
    "    except:\n",
    "        print(\"Directory \",fileName,\" not found to delete.\")\n",
    "\n",
    "#Create directories for training, validation and test data\n",
    "subdirs  = ['train/', 'validation/', 'test/']\n",
    "for subdir in subdirs:\n",
    "    labeldirs = ['CT_COVID', 'CT_NonCOVID']\n",
    "    for labldir in labeldirs:\n",
    "        newdir = subdir + labldir\n",
    "        os.makedirs(newdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "46DuSbAeEuiB"
   },
   "outputs": [],
   "source": [
    "# Copy randomly files from directories to working directory separated in train, validation and set with a likelihood of 70& train, 20% validation, 10% test\n",
    "train_size = 0        \n",
    "validation_size = 0\n",
    "test_size = 0 \n",
    "path_COVID = os.path.join('./data/input/covidct/CT_COVID/')\n",
    "path_NonCOVID = os.path.join('./data/input/covidct/CT_NonCOVID/')\n",
    "covid_images = glob(os.path.join(path_COVID,\"*.png\"))\n",
    "covid_images.extend(glob(os.path.join(path_COVID,\"*.jpg\")))\n",
    "noncovid_images = glob(os.path.join(path_NonCOVID,\"*.png\"))\n",
    "noncovid_images.extend(glob(os.path.join(path_NonCOVID,\"*.jpg\")))\n",
    "\n",
    "for filename in covid_images: #copy to positive directory\n",
    "    randNumber = random.uniform(0, 1)\n",
    "    if(randNumber<0.7) : #train set\n",
    "        shutil.copy(filename, \"./data/working/train/CT_COVID/\")\n",
    "        train_size = train_size + 1\n",
    "    elif(randNumber<0.9) : #validation set\n",
    "        shutil.copy(filename, \"./data/working/validation/CT_COVID/\")\n",
    "        validation_size = validation_size + 1\n",
    "    else: #test set\n",
    "        shutil.copy(filename, \"./data/working/test/CT_COVID/\")\n",
    "        test_size = test_size +1\n",
    "        \n",
    "for filename in noncovid_images: #copy to negative directory\n",
    "    randNumber = random.uniform(0, 1)\n",
    "    if(randNumber<0.7) : #train set\n",
    "        shutil.copy(filename, \"./data/working/train/CT_NonCOVID/\")\n",
    "        train_size = train_size + 1\n",
    "    elif(randNumber<0.9) : #validation set\n",
    "        shutil.copy(filename, \"./data/working/validation/CT_NonCOVID/\")\n",
    "        validation_size = validation_size + 1\n",
    "    else: #test set\n",
    "        shutil.copy(filename, \"./data/working/test/CT_NonCOVID/\")\n",
    "        test_size = test_size +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "W99NPlq_F_9X"
   },
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "H7ZCyG08GAo6"
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "91bef78HGKzo"
   },
   "outputs": [],
   "source": [
    "# Some variables\n",
    "IMG_HEIGHT = 150\n",
    "IMG_WIDTH = 150\n",
    "train_dir = os.path.join('./data/working/train/')\n",
    "validation_dir = os.path.join('./data/working/validation')\n",
    "test_dir = os.path.join('./data/working/test/')\n",
    "batch_size = 16\n",
    "epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "80qXNSanGOC4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "Found 510 images belonging to 2 classes.\n",
      "Validation set:\n",
      "Found 144 images belonging to 2 classes.\n",
      "Test set:\n",
      "Found 92 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set:\")\n",
    "training_set = train_datagen.flow_from_directory(batch_size=batch_size,\n",
    "                                                           directory=train_dir,\n",
    "                                                           shuffle=True,\n",
    "                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                           class_mode='binary')\n",
    "print(\"Validation set:\")\n",
    "validation_set = train_datagen.flow_from_directory(batch_size=batch_size,\n",
    "                                                           directory=validation_dir,\n",
    "                                                           shuffle=True,\n",
    "                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                           class_mode='binary')\n",
    "print(\"Test set:\")\n",
    "test_set = test_datagen.flow_from_directory(batch_size=batch_size,\n",
    "                                                              directory=test_dir,\n",
    "                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                              class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "LynzQ8T-GT7w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 72, 72, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 34, 34, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 17, 17, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9248)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                591936    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 611,393\n",
      "Trainable params: 611,393\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Creation of the CNN\n",
    "\n",
    "classifier = Sequential()                                                                           \n",
    "# Step 1 - Convolution\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (IMG_HEIGHT, IMG_WIDTH, 3), activation = 'relu'))\n",
    "\n",
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding a second convolutional layer\n",
    "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding a second convolutional layer\n",
    "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(units = 64, activation = 'relu'))\n",
    "classifier.add(Dropout(0.4))\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "print(classifier.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "foIxzhNuGWmQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "510/510 [==============================] - 154s 301ms/step - loss: 0.5824 - accuracy: 0.6774 - val_loss: 0.5063 - val_accuracy: 0.7808\n",
      "Epoch 2/15\n",
      "510/510 [==============================] - 142s 279ms/step - loss: 0.3607 - accuracy: 0.8336 - val_loss: 0.4118 - val_accuracy: 0.8138\n",
      "Epoch 3/15\n",
      "510/510 [==============================] - 139s 273ms/step - loss: 0.2199 - accuracy: 0.9027 - val_loss: 0.7641 - val_accuracy: 0.7960\n",
      "Epoch 4/15\n",
      "510/510 [==============================] - 148s 290ms/step - loss: 0.1482 - accuracy: 0.9402 - val_loss: 0.6863 - val_accuracy: 0.8003\n",
      "Epoch 5/15\n",
      "510/510 [==============================] - 144s 282ms/step - loss: 0.1063 - accuracy: 0.9580 - val_loss: 0.6095 - val_accuracy: 0.7908\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "#epochs = 2; #Uncomment to test quick\n",
    "#set early stopping criteria\n",
    "from keras.callbacks import EarlyStopping\n",
    "epochsWithOutImprovement = 3 #this is the number of epochs with no improvment after which the training will stop\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=epochsWithOutImprovement, verbose=1)\n",
    "                               \n",
    "history = classifier.fit_generator(training_set,\n",
    "                         steps_per_epoch = train_size,\n",
    "                         epochs = epochs,\n",
    "                         validation_data = validation_set,\n",
    "                         validation_steps = validation_size,\n",
    "                         callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "THmyW5m9GbBI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy with TEST SET: 71.67%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = classifier.evaluate_generator(test_set,steps=test_size)\n",
    "print('Testing Accuracy with TEST SET: {:.2f}%'.format(test_accuracy[1] * 100))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "COVID_CT_.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
